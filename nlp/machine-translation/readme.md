# Machine Translation
Machine Translation is a field, which goal is to automate translation between languages. It was of interest to computer scientist even in the early stages of computing (~1950s).

The first systems were rule-based. An example is SYSTRAN (one of the oldest machine translation companies). It was used in Google Translate until 2007. Later on, people realized that statistical systems coul be quite successful. Google adopted it in 2007, but it's origins date back at least to 2000, when a group of IBM researchers developed 'Candide' system (https://www.aclweb.org/anthology/H94-1028.pdf). Among the authors of this paper were Peter F. Brown and Robert Mercer, which later run Reneissance Technologies (one of the most known quantitative trading companies, see "The Man Who Solved the Market: How Jim Simons Launched the Quant Revolution" book). Even though, that statistical approach is now taken for granted, it was a novel idea to use probabilistic modeling at the time. The idea of using data for translation was improved later on, when neural translation was introduced. Of course, this can be thought of as just another technical way of doing statistical machine translation (we just approximate probability distributions with neural networks). The general idea was outlined in "Sequence to Sequence Learning with Neural Network" by Sutskever, Vinyals and Le (2014). In 2016 Google introduced it's Neural Machine Translation system (GMNT, https://arxiv.org/abs/1609.08144). It was then improved, the main improvement being exchanging RNNs (LSTMs) with transformers ("Attention Is All You Need", https://arxiv.org/abs/1706.03762).

## IBM's Candide
Candide focused on translating French to English. In other words, the goal was to estimate p(e|f), where e is an english sentence and f is a french sentence. Rewriting it as p(f|e)*p(e) suggests using two models: a translation model (p(f|e)) and a language model (p(e)). They use a smoothed trigram model and a link grammar model for a language model (I do not know what is a link grammar model, but the authors describe it as 'trainable, probabilistic grammar that attempts to caputre all the information present in the trigram model, and also to make the long-range connections among words needed to advance beyond it'). Translation model is trained with EM (e.g. with hidden alignment 'a' as a latent). There is a lot of detail described in a paer, which I will not describe here. However, it seems, that this model required much more work and clever ideas than simple neural network models popular today.

## Neural Machine Translation systems
Just to give a quick ovverview of the field. As mentioned before, sequence to sequence learning, which is a basic idea of neural machine translation was introduced in "Sequence to Sequence Learning with Neural Network ("https://arxiv.org/abs/1409.3215). Later on, it was refined by introducing attention mechanism ("Neural Machine Translation by Jointly Learning to Align and Translate": Bahdanau, Cho, Bengio). 

Even when introducing this mechanism, this method was actually worse than phrase-based translation! "Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation" (https://arxiv.org/abs/1609.08144) describe how to effectively design large scale neural machine translation system. Besides describing engineering of the system (how to effectively train and use a model with large number of layers, low-precision arithmetic) it introduced WordPiece tokenization, which is now a standard (for example in Bert model). This paper also makes you realize how difficult it is to design performant, and accurace neural architecture. To be effective, the model has to have a lot of LSTM layers (each on a different GPU), use residual connections etc. 

The work on Machine Translation continued, one important change was to use Convolutional Neural Networks (both in encoder and decoder). This was done in ByteNet by DeepMind (https://arxiv.org/abs/1610.10099) and Conv2S by FAIR (https://arxiv.org/abs/1705.03122). Using convolutions allowed for a significant speed up due to bigger parallelization.

Finally, transformer architecture outperformed them all ("Attention Is All You Need", https://arxiv.org/abs/1706.03762). It also did it in orders of magnitute shorter amount of time. It took time to incorporate it in production, there were some improvements when using backtranslation for data augmentation (https://arxiv.org/abs/1808.09381) but the quality of predictions is quite good. Very important observations however, is that Google Translate actually uses RNN in the decoder! It is faster and most of the improvements come from transformer decoder (https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html).

Now, I can show my dad an article translated from English into Polish and the translation is accurate and very fluent (with some minor mistakes). Is the problem solved? For English-Polish translations seems to be good enough (although it depends on the level of quality we want to achieve). I don't know about other languages. Also, there is a more general problem of world and domain knowledge required to understand a text. Sometimes it is not possible to translate a sentence based just on statistical patterns. Statistical systems are good enough for getting an understanding of a text, but do not produce high-quality texts that professsinal translators do.
