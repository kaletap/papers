Machine Translation is a field, which goal is to automate translation between languages. It was of interest to computer scientist even in the early stages of computing (~1950s).

The first systems were rule-based. An example is SYSTRAN (one of the oldest machine translation companies). It was used in Google Translate until 2007. Later on, people realized that statistical systems coul be quite successful. Google adopted it in 2007, but it's origins date back to 2000, when a group of IBM researchers developed 'Candide' system (https://www.aclweb.org/anthology/H94-1028.pdf). Among the authors of this paper were Peter F. Brown and Robert Mercer, which later run Reneissance Technologies (one of the most known quantitative trading companies). Even though, that statistical approach is now taken for granted, it was a novel idea to use probabilistic modeling at the time. The idea of using data for translation was improved later on, when neural translation was introduced. Of course, this can be thought of as just another technical way of doing statistical machine translation (we just approximate probability distributions with neural networks). The general idea was outlined in "Sequence to Sequence Learning with Neural Network" by Sutskever, Vinyals and Le (2014). In 2016 Google introduced it's Neural Machine Translation system (GMNT, https://arxiv.org/abs/1609.08144). It was then improved, the main improvement being exchanging RNNs (LSTMs) with transformers ("Attention Is All You Need", https://arxiv.org/abs/1706.03762).


Candide focused on translating French to English. In other words, the goal was to estimate p(e|f), where e is an english sentence and f is a french sentence. Rewriting it as p(f|e)*p(e) suggests using two models: a translation model (p(f|e)) and a language model (p(e)). They use a smoothed trigram model and a link grammar model for a language model (I do not know what is a link grammar model, but the authors describe it as 'trainable, probabilistic grammar that attempts to caputre all the information present in the trigram model, and also to make the long-range connections among words needed to advance beyond it'). Translation model is trained with EM (e.g. with hidden alignment 'a' as a latent). There is a lot of detail described in a paer, which I will not describe here. However, it seems, that this model required much more work and clever ideas than simple neural network models popular today.


